{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Heart Disease Using Scikit-Learn\n",
    "\n",
    "This notebook uses a logistic regression scikit-learn pipeline to predict heart disease in a patient given a set of clinical measurements.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "In a statement,\n",
    "> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n",
    "\n",
    "## Data\n",
    "\n",
    "The original data came from the Cleavland data (processed.cleveland.data) from the UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "\n",
    "There is also a version of it available on Kaggle, but there are issues with the `thal` field not matching the original. https://www.kaggle.com/ronitf/heart-disease-uci\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll deploy the pipeline.\n",
    "\n",
    "## Features\n",
    "  \n",
    "1. age - age in years\n",
    "2. sex - (1 = male; 0 = female)\n",
    "3. cp - chest pain type\n",
    "    * 1 - typical angina\n",
    "    * 2 - atypical angina\n",
    "    * 3 - non-anginal pain\n",
    "    * 4 - asymptomatic\n",
    "4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "5. chol - serum cholestoral in mg/dl\n",
    "6. fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "7. restecg - resting electrocardiographic results\n",
    "    * 0 - normal\n",
    "    * 1 - having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \n",
    "    * 2 - showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "8. thalach - maximum heart rate achieved\n",
    "9. exang - exercise induced angina (1 = yes; 0 = no)\n",
    "10. oldpeak - ST depression induced by exercise relative to rest\n",
    "11. slope - the slope of the peak exercise ST segment\n",
    "    * 1 - upsloping\n",
    "    * 2 - flat\n",
    "    * 3 - downsloping\n",
    "12. ca - number of major vessels (0-3) colored by flourosopy\n",
    "13. thal\n",
    "    * 3 - normal\n",
    "    * 6 - fixed defect\n",
    "    * 7 - reversable defect\n",
    "14. target - 0,1,2,3,4 (where > 0 indicates heart disease)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the tools\n",
    "\n",
    "We're going to use standard tools in the Data Scientist's toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the tools we need\n",
    "# Regular EDA and plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Model from Scikit-Learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model Evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google Cloud will require scikit-learn version 0.22.1. Let's double check that we have the right version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(f\"sklearn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We've already downloaded the data set and saved it in the data folder. If you would like to download directly, the data set can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "             \"thalach\", \"exang\", \"oldpeak\", \"slope\",\"ca\", \"thal\", \"target\"]\n",
    "\n",
    "df = pd.read_csv(\"../data/processed.cleveland.data\", names = col_names, na_values = \"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations Required\n",
    "There are a few data transformations that need to take place in order to\n",
    "Based on the data type descriptions on the UCI Machine Learning Repository, we will convert some of the fields to be categorical. Namely,  \n",
    "* Chest Pain Type (cp)\n",
    "* Resting Electrocardiographic Results (restecg)\n",
    "* thal\n",
    "\n",
    "We'll create a dictionary and replace each of the numeric values with its categorical counterpart. We also need to re-code our target so that it is binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_label_conversion(df):\n",
    "    \n",
    "    cp_dict = {1: \"typical angina\",\n",
    "               2: \"atypical angina\",\n",
    "               3: \"non-anginal pain\", \n",
    "               4: \"asymptomatic\"}\n",
    "\n",
    "    restecg_dict = {0: \"normal\", \n",
    "                    1: \"wave abnormality\", \n",
    "                    2: \"ventricular hypertrophy\"}\n",
    "\n",
    "    thal_dict = {3 : \"normal\",\n",
    "                 6 : \"fixed defect\",\n",
    "                 7 : \"reversable defect\"}\n",
    "    \n",
    "    df[\"cp\"].replace(cp_dict, inplace = True)\n",
    "    df[\"restecg\"].replace(restecg_dict, inplace = True)\n",
    "    df[\"thal\"].replace(thal_dict, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X and y\n",
    "X = df.drop(\"target\", axis = 1)\n",
    "y = (df[\"target\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Next, we'll try to improve the baseline `LogisticRegression` model using GridSearchCV. We'll also use the pipeline operator to string together a simple pre-processing step and our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'pass_y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-3fd68de66e90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbin_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"impute\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"most_frequent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtarget_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"binarize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinarize_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m preprocessor = ColumnTransformer(transformers = [('cat', cat_transformer, cat_vars),\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'pass_y'"
     ]
    }
   ],
   "source": [
    "# Let's split up our features into three different groups that will undergo separate transformations:\n",
    "cat_vars = [\"cp\",\"restecg\",\"thal\"]\n",
    "num_vars = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"slope\",\"ca\"]\n",
    "bin_vars = [\"sex\", \"fbs\", \"exang\"]\n",
    "\n",
    "\n",
    "cat_transformer = Pipeline(steps = [(\"cat_encoding\", FunctionTransformer(num_to_label_conversion)),\n",
    "                                    (\"impute\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                    (\"ohe\", OneHotEncoder())])\n",
    "                                   \n",
    "\n",
    "num_transformer = Pipeline(steps = [(\"impute\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                    (\"scaler\", StandardScaler())])\n",
    "\n",
    "bin_transformer = Pipeline(steps = [(\"impute\", SimpleImputer(strategy = \"most_frequent\"))])\n",
    "\n",
    "target_transformer = Pipeline(steps = [(\"binarize\", FunctionTransformer(binarize_y, pass_y = True))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [('cat', cat_transformer, cat_vars),\n",
    "                                                  ('num', num_transformer, num_vars),\n",
    "                                                  ('bin', bin_transformer, bin_vars)],\n",
    "                                  remainder = \"drop\")\n",
    "\n",
    "log_model_pipeline = Pipeline(steps = [\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"model\", LogisticRegression())])\n",
    "\n",
    "# Different hyperparameters for the LogisticRegression model\n",
    "log_param_grid = {\"model__C\": np.logspace(-4, 4, 30)}\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_log_model = GridSearchCV(log_model_pipeline, log_param_grid, cv = 5)\n",
    "gs_log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8511904761904763"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check best cross-validated score\n",
    "gs_log_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': 0.20433597178569418}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the best hyperparameters\n",
    "gs_log_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.819672131147541"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the grid search LogisticRegression model\n",
    "gs_log_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29,  4],\n",
       "       [ 7, 21]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = gs_log_model.predict(X_test)\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gs_log_model_v1.pkl', 'wb') as model_file:\n",
    "    pickle.dump(gs_log_model, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
