{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Scripts for Model Deployment on the Google Cloud\n",
    "**Author:** Robert Smith  \n",
    "**Date:** 06-21-2020\n",
    "\n",
    "This final notebook covers the last (and most challenging) stage of the CRISP-DM lifecycle, model deployment. In order to deploy a model using the AI Platform, a few files will be created:  \n",
    "\n",
    "> * **preprocess.py:** A python module that contains a data pre-processing class to transform the raw data into a form suitable for a scikit-learn pipeline.  \n",
    "> * **gs_log_model.pkl:** A pickled scikit-learn machine learning pipeline, using the `preprocess` module created above to prepare and model the data.  \n",
    "> * **preprocessor.pkl** A pickled `HeartDiseaseTransformer` preprocessor class from the preprocess.py.  \n",
    "> * **predictor.py:** A python module that uses the preprocessor and scikit-learn machine learning pipeline as inputs to transform and predict on new observations.  \n",
    ">*  **heart_disease_classification-0.1.tar.gz:** A source distribution containing the `preprocess.py` and `predictor.py` scripts.  \n",
    "\n",
    "After all of these files are created, we'll access the command line here in Jupyter by prefixing our code with a bang (!) operator to push the source distribution and the pickle files to the Google Cloud Platform. \n",
    "\n",
    "As a final step - we'll use the command line and Google's Python API client to pass the deployed model a couple patient observations that we'd like to estimate heart disease probability for.\n",
    "\n",
    "**NOTE:** You'll need to have your GCP account, billing, service account, APIs, and project set-up, and the Google SDK installed and initialized prior to deploying your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Preprocessor Module\n",
    "First, we'll create data pre-processor class that takes care of a couple data pre-processing steps outside of scikit-learn, including the translation of a few features from numeric into categorical (which we'll subsequently encode using one-hot encoding in a pipeline) and transform the numeric target feature into a binary indicator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class HeartDiseaseTransformer():\n",
    "    def __init__(self):\n",
    "        self._cp_dict = {1: \"typical angina\",\n",
    "                         2: \"atypical angina\",\n",
    "                         3: \"non-anginal pain\", \n",
    "                         4: \"asymptomatic\"}\n",
    "        \n",
    "        self._restecg_dict = {0: \"normal\", \n",
    "                              1: \"wave abnormality\", \n",
    "                              2: \"ventricular hypertrophy\"}\n",
    "        \n",
    "        self._thal_dict = {3 : \"normal\",\n",
    "                           6 : \"fixed defect\",\n",
    "                           7 : \"reversable defect\"}\n",
    "    \n",
    "    def preprocess_X(self, data):\n",
    "        data[\"cp\"].replace(self._cp_dict, inplace = True)\n",
    "        data[\"restecg\"].replace(self._restecg_dict, inplace = True)\n",
    "        data[\"thal\"].replace(self._thal_dict, inplace = True)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def preprocess_y(self, data):\n",
    "        data = (data > 0).astype(int)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Pickle Model & Preprocessor\n",
    "Using the pre-processing module just created above, this section re-runs the modeling approach in the previous notebook and pickles the resulting model and the instantiated pre-processing class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular EDA and plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Models from Scikit-Learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Other functions needed from Scikit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import class just written\n",
    "from preprocess import HeartDiseaseTransformer\n",
    "\n",
    "# Export final model\n",
    "import pickle\n",
    "\n",
    "# Import raw data\n",
    "col_names = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "             \"thalach\", \"exang\", \"oldpeak\", \"slope\",\"ca\", \"thal\", \"target\"]\n",
    "\n",
    "df = pd.read_csv(\"../data/processed.cleveland.data\", names = col_names, na_values = \"?\")\n",
    "\n",
    "# Split data into X and y\n",
    "X = df.drop(\"target\", axis = 1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Use pre-processor\n",
    "hdt = HeartDiseaseTransformer()\n",
    "X = hdt.preprocess_X(X)\n",
    "y = hdt.preprocess_y(y)\n",
    "\n",
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 123)\n",
    "\n",
    "# Create Pipeline & Train Model\n",
    "# Let's split up our features into three different groups that will undergo separate transformations:\n",
    "cat_vars = [\"cp\",\"restecg\",\"thal\"]\n",
    "num_vars = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"slope\",\"ca\"]\n",
    "bin_vars = [\"sex\", \"fbs\", \"exang\"]\n",
    "\n",
    "\n",
    "cat_transformer = Pipeline(steps = [(\"impute\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                    (\"ohe\", OneHotEncoder())])\n",
    "                                   \n",
    "\n",
    "num_transformer = Pipeline(steps = [(\"impute\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "                                    (\"scaler\", PowerTransformer())]) # Here we are using a power-transform instead.\n",
    "\n",
    "bin_transformer = Pipeline(steps = [(\"impute\", SimpleImputer(strategy = \"most_frequent\"))])\n",
    "\n",
    "preprocessor_pt = ColumnTransformer(transformers = [('cat', cat_transformer, cat_vars),\n",
    "                                                  ('num', num_transformer, num_vars),\n",
    "                                                  ('bin', bin_transformer, bin_vars)],\n",
    "                                  remainder = \"drop\")\n",
    "\n",
    "log_model_pipeline = Pipeline(steps = [\n",
    "    (\"preprocessing\", preprocessor_pt),\n",
    "    (\"model\", LogisticRegression())])\n",
    "\n",
    "# Create a hyperparameter grid for Logistic Regression\n",
    "np.random.seed(42)\n",
    "param_grid = {\"model__penalty\": [\"l2\", \"l1\"],\n",
    "                \"model__C\": np.logspace(-4, 4, 30), \n",
    "                \"model__solver\" : [\"liblinear\"]}\n",
    "\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_log_model = GridSearchCV(log_model_pipeline, param_grid, cv = 10, iid = False)\n",
    "gs_log_model.fit(X_train, y_train)\n",
    "\n",
    "# Save Model\n",
    "with open('../model/log_model_v1.pkl', 'wb') as model_file:\n",
    "    pickle.dump(gs_log_model, model_file)\n",
    "    \n",
    "# Save pre-processor\n",
    "with open ('../model/preprocessor.pkl', 'wb') as preprocessor_file:\n",
    "    pickle.dump(hdt, preprocessor_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Predictor Module\n",
    "The AI Platform also expects a predictor module for deploying a custom prediction routine. This module contains a class that instantiates the pre-processor and scikit-learn machine learning pipeline and uses these together to predict the probability of heart disease on new, unseen observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predictor.py \n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class HeartDiseasePredictor(object):\n",
    "    def __init__(self, model, preprocessor):\n",
    "        self._model = model\n",
    "        self._preprocessor = preprocessor\n",
    "\n",
    "    def predict(self, instances, **kwargs):\n",
    "        instances = pd.DataFrame(instances, \n",
    "             columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "             \"thalach\", \"exang\", \"oldpeak\", \"slope\",\"ca\", \"thal\"])\n",
    "        preprocessed_inputs = self._preprocessor.preprocess_X(instances)\n",
    "        outputs = self._model.predict_proba(preprocessed_inputs)\n",
    "        return [str(np.round(p[1],2)*100)+\"%\" for p in outputs]\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "        model_path = os.path.join(model_dir, 'log_model_v1.pkl')\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        preprocessor_path = os.path.join(model_dir, 'preprocessor.pkl')\n",
    "        with open(preprocessor_path, 'rb') as f:\n",
    "            preprocessor = pickle.load(f)\n",
    "        \n",
    "        return cls(model, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the new module by passing it a couple of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['31.0%', '15.0%']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predictor import HeartDiseasePredictor\n",
    "instances = [\n",
    "    {\n",
    "      \"age\": 63.0,\n",
    "      \"sex\": 1.0,\n",
    "      \"cp\": 1.0,\n",
    "      \"trestbps\": 145.0,\n",
    "      \"chol\": 233.0,\n",
    "      \"fbs\": 1.0,\n",
    "      \"restecg\": 2.0,\n",
    "      \"thalach\": 150.0,\n",
    "      \"exang\": 0.0,\n",
    "      \"oldpeak\": 2.3,\n",
    "      \"slope\": 3.0,\n",
    "      \"ca\": 0.0,\n",
    "      \"thal\": 6.0\n",
    "    },\n",
    "        {\n",
    "      \"age\": 63.0,\n",
    "      \"sex\": 0.0,\n",
    "      \"cp\": 1.0,\n",
    "      \"trestbps\": 145.0,\n",
    "      \"chol\": 233.0,\n",
    "      \"fbs\": 1.0,\n",
    "      \"restecg\": 2.0,\n",
    "      \"thalach\": 150.0,\n",
    "      \"exang\": 0.0,\n",
    "      \"oldpeak\": 2.3,\n",
    "      \"slope\": 3.0,\n",
    "      \"ca\": 0.0,\n",
    "      \"thal\": 6.0\n",
    "    }\n",
    "  ]\n",
    "hdp = HeartDiseasePredictor.from_path(\"../model/\")\n",
    "hdp.predict(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The module is able to return a string of the heart disease probability. The second observation has the same clinical measurements as the first obervation except for the sex. The resulting probabilities indicate that a women has a 16% lower risk of heart disease compared to a man with the same clinical measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing that needs to be done before deployment is to package the `preprocess.py` and `predictor.py` into a source distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py \n",
    "\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='heart_disease_classification',\n",
    "    version='0.1',\n",
    "    scripts=['predictor.py', 'preprocess.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create source distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\r\n",
      "running egg_info\r\n",
      "creating heart_disease_classification.egg-info\r\n",
      "writing heart_disease_classification.egg-info/PKG-INFO\r\n",
      "writing dependency_links to heart_disease_classification.egg-info/dependency_links.txt\r\n",
      "writing top-level names to heart_disease_classification.egg-info/top_level.txt\r\n",
      "writing manifest file 'heart_disease_classification.egg-info/SOURCES.txt'\r\n",
      "reading manifest file 'heart_disease_classification.egg-info/SOURCES.txt'\r\n",
      "writing manifest file 'heart_disease_classification.egg-info/SOURCES.txt'\r\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\r\n",
      "\r\n",
      "running check\r\n",
      "warning: check: missing required meta-data: url\r\n",
      "\r\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\r\n",
      "\r\n",
      "creating heart_disease_classification-0.1\r\n",
      "creating heart_disease_classification-0.1/heart_disease_classification.egg-info\r\n",
      "copying files to heart_disease_classification-0.1...\r\n",
      "copying predictor.py -> heart_disease_classification-0.1\r\n",
      "copying preprocess.py -> heart_disease_classification-0.1\r\n",
      "copying setup.py -> heart_disease_classification-0.1\r\n",
      "copying heart_disease_classification.egg-info/PKG-INFO -> heart_disease_classification-0.1/heart_disease_classification.egg-info\r\n",
      "copying heart_disease_classification.egg-info/SOURCES.txt -> heart_disease_classification-0.1/heart_disease_classification.egg-info\r\n",
      "copying heart_disease_classification.egg-info/dependency_links.txt -> heart_disease_classification-0.1/heart_disease_classification.egg-info\r\n",
      "copying heart_disease_classification.egg-info/top_level.txt -> heart_disease_classification-0.1/heart_disease_classification.egg-info\r\n",
      "Writing heart_disease_classification-0.1/setup.cfg\r\n",
      "creating dist\r\n",
      "Creating tar archive\r\n",
      "removing 'heart_disease_classification-0.1' (and everything under it)\r\n"
     ]
    }
   ],
   "source": [
    "! python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Custom Prediction Routine to the Cloud\n",
    "Now we are ready to deploy the model pipeline, including the data pre-processor, to the cloud! We'll first make sure our GCP environment has the right service account credentials. You'll want to download the key file (in json format) for a GCP service account that will have access to your model and provide that as a file path:  \n",
    "**Example:**  \n",
    "\n",
    "`%env GOOGLE_APPLICATION_CREDENTIALS <path/to/credientials.json>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=/Users/RobertSmith/data-science-projects/gcp-key/heart_disease_classification_key.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS /Users/RobertSmith/data-science-projects/gcp-key/heart_disease_classification_key.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to set the GCP parameters needed to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"heart-disease-classification\" # Name of project in GCP\n",
    "REGION = \"us-central1\" # Where we will create our storage bucket and deploy our model\n",
    "BUCKET_NAME = \"heart-disease-classification\" # Name of our storage bucket where will will store our code\n",
    "MODEL_NAME = 'HeartDiseasePredictor' # Name of the model to be deployed\n",
    "VERSION_NAME = 'v1' # Version of the MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "# Set our environment to the right project...\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://heart-disease-classification/...\r\n"
     ]
    }
   ],
   "source": [
    "# Create Bucket\n",
    "! gsutil mb -l $REGION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../model/log_model_v1.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://../model/preprocessor.pkl [Content-Type=application/octet-stream]...\n",
      "- [2 files][ 27.6 KiB/ 27.6 KiB]                                                \n",
      "Operation completed over 2 objects/27.6 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Copy our pickled model and data pre-processor to the newly created storage bucket in the model folder\n",
    "! gsutil cp ../model/log_model_v1.pkl ../model/preprocessor.pkl gs://heart-disease-classification/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./dist/heart_disease_classification-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Copy our source distribution to the storage bucket\n",
    "! gsutil cp ./dist/heart_disease_classification-0.1.tar.gz gs://heart-disease-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/heart-disease-classification/models/HeartDiseasePredictor].\n"
     ]
    }
   ],
   "source": [
    "# Create a model in the region chosen above\n",
    "! gcloud ai-platform models create $MODEL_NAME \\\n",
    "  --regions $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "# Create a model version with the same configuration as the local data science environment. This is why it is\n",
    "# sooo important to make sure that the same package versions installed locally are available on the AI Platform\n",
    "\n",
    "# --quiet automatically installs the beta component if it isn't already installed \n",
    "! gcloud --quiet beta ai-platform versions create $VERSION_NAME \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --runtime-version 1.15 \\\n",
    "  --python-version 3.7 \\\n",
    "  --origin gs://$BUCKET_NAME/model/ \\\n",
    "  --package-uris gs://$BUCKET_NAME/heart_disease_classification-0.1.tar.gz \\\n",
    "  --prediction-class predictor.HeartDiseasePredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model Predictions\n",
    "We can use the model in two different ways:\n",
    "* Via the command line\n",
    "* Google's Python API Client\n",
    "\n",
    "### Command Line Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to two clinical observations\n",
    "INPUT_DATA_FILE = \"../data/observations_for_prediction.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "predictions[0]: 31.0%\n",
      "predictions[1]: 15.0%\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform predict --model $MODEL_NAME  \\\n",
    "                   --version $VERSION_NAME \\\n",
    "                   --json-request $INPUT_DATA_FILE \\\n",
    "                   --format text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31.0%', '15.0%']\n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "      \"age\" : 63.0,\n",
    "      \"sex\" : 1.0,\n",
    "      \"cp\" : 1.0,\n",
    "      \"trestbps\" : 145.0,\n",
    "      \"chol\" : 233.0,\n",
    "      \"fbs\" : 1.0,\n",
    "      \"restecg\" : 2.0,\n",
    "      \"thalach\" : 150.0,\n",
    "      \"exang\" : 0.0,\n",
    "      \"oldpeak\" : 2.3,\n",
    "      \"slope\" : 3.0,\n",
    "      \"ca\" : 0.0,\n",
    "      \"thal\" : 6.0\n",
    "    },\n",
    "    {\n",
    "      \"age\" : 63.0,\n",
    "      \"sex\" : 0.0,\n",
    "      \"cp\" : 1.0,\n",
    "      \"trestbps\" : 145.0,\n",
    "      \"chol\" : 233.0,\n",
    "      \"fbs\" : 1.0,\n",
    "      \"restecg\" : 2.0,\n",
    "      \"thalach\" : 150.0,\n",
    "      \"exang\" : 0.0,\n",
    "      \"oldpeak\" : 2.3,\n",
    "      \"slope\" : 3.0,\n",
    "      \"ca\" : 0.0,\n",
    "      \"thal\" : 6.0\n",
    "    }\n",
    "  ]\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}/versions/{}'.format(PROJECT_ID, MODEL_NAME, VERSION_NAME)\n",
    "\n",
    "response = service.projects().predict(\n",
    "    name=name,\n",
    "    body={'instances': instances}\n",
    ").execute()\n",
    "\n",
    "if 'error' in response:\n",
    "    raise RuntimeError(response['error'])\n",
    "else:\n",
    "  print(response['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tear-down AI Platform Resources\n",
    "To avoid incurring ongoing charges, the following commands will delete the model (and version) and associated versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete version resource\n",
    "#! gcloud ai-platform versions delete $VERSION_NAME --quiet --model $MODEL_NAME \n",
    "\n",
    "# Delete model resource\n",
    "#! gcloud ai-platform models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "#! gsutil -m rm -r gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
